{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KN6d7vf4etqd"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y3_TuM4aetql"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OpUf9aJJetqn"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZCO3b19Yetqw"
   },
   "source": [
    "#######################################################################\n",
    "\n",
    "################## By Imranul Ashrafi #################################\n",
    "\n",
    "################## By Ifty Md. Rezwan #################################\n",
    "\n",
    "#######################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NrxxtCeTetqx"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dv3kD3g2etq4"
   },
   "source": [
    "## This notebook will contain the model and it will divided into 4 modules chiefly.\n",
    "\n",
    "### 1)Utility Functions\n",
    "\n",
    "### 2)Dataset\n",
    "\n",
    "### 3)Model (The skeleton is always fun to build. How the data pours through the hourglass.....)\n",
    "\n",
    "### 4)(Training) (FUNNNNN Part, But always pray to GOD, it goes right.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MakcooyRetq4"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4BYJ5o6petq-"
   },
   "source": [
    " # Also remember to install the dependencies or nothing will work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wxh-0Nqoetq_"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rySj9CcEetrE"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ge5L7elnetrJ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L3lQ887petrN"
   },
   "source": [
    "# Utility Functions "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Auv225T8etrO"
   },
   "source": [
    "### (Hard to write and not the fun part)\n",
    "### (Required for data encoding and decoding)\n",
    "### (Also some necessary metric helper functions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "V4zjeACEetrP"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gU6PhJpmetrU"
   },
   "outputs": [],
   "source": [
    "#necessary imports for utility functions\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "62jqzeSretrZ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wjXKIy_8etrd"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#####################METRICS Functions############################################################\n",
    "\n",
    "\n",
    "\n",
    "#####################Function to calculate Average Edit Distance (NOT word error rate)############\n",
    "\n",
    "\n",
    "#required for word error rate (WER)\n",
    "#pip install python-Levenshtein\n",
    "#preferred , pip install python-Levenshtein==0.12.0\n",
    "import Levenshtein as Lev\n",
    "\n",
    "#WER functions\n",
    "def compute_wer(predictions, labels):\n",
    "    total_dist = 0\n",
    "    \n",
    "    #Check if prediction and original label are same\n",
    "    assert len(predictions) == len(labels)\n",
    "\n",
    "    for i in range(len(predictions)):\n",
    "        total_dist += Lev.distance(predictions[i], labels[i])\n",
    "        #আমাার\n",
    "        #তাামাার\n",
    "        #abcd\n",
    "        #abdfr\n",
    "        #edit distance 1\n",
    "\n",
    "    word_error_rate = ( total_dist/len(predictions) )\n",
    "\n",
    "    return word_error_rate\n",
    "\n",
    "####################Metric which counts the number of words that absolutely match####################\n",
    "\n",
    "#Absolute matching function\n",
    "def absolute_word_match(predictions, labels):\n",
    "    count_correct = 0\n",
    "    for x, y in zip(predictions, labels):\n",
    "        #print(x)\n",
    "        #print(y)\n",
    "        if(x==y):\n",
    "            count_correct += 1\n",
    "    print(\"Absolute word match count is {}\".format(count_correct) )\n",
    "\n",
    "    return count_correct\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xaC9x7fOetri"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gJUw0Fi7etrm"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "###############################################Preprocessing functions#####################################\n",
    "\n",
    "\n",
    "\n",
    "def preprocess_data(data_dir):\n",
    "    grapheme_dict = {}\n",
    "    labels = []\n",
    "    words = []\n",
    "    lengths = []\n",
    "    count = 2\n",
    "    filenames = os.listdir(data_dir)\n",
    "    filenames = sorted(filenames, key=lambda x: int(x.split('_')[0]))\n",
    "\n",
    "    grapheme_dict['<eow>'] = 1\n",
    "\n",
    "    for i, name in enumerate(filenames):\n",
    "        curr_word = name.split('_')[1][:-4]\n",
    "        # print(curr_word)\n",
    "        curr_label = []\n",
    "        words.append(curr_word)\n",
    "        graphemes = extract_graphemes(curr_word)\n",
    "        # if 'স্ক্র্' in graphemes:\n",
    "        #     print(curr_word)\n",
    "        #     print(graphemes)\n",
    "        for grapheme in graphemes:\n",
    "            if grapheme not in grapheme_dict:\n",
    "                grapheme_dict[grapheme] = count\n",
    "                curr_label.append(count)\n",
    "                count += 1\n",
    "            else:\n",
    "                curr_label.append(grapheme_dict[grapheme])\n",
    "        lengths.append(len(curr_label))\n",
    "        labels.append(curr_label)\n",
    "    \n",
    "\n",
    "    inv_grapheme_dict = {v: k for k, v in grapheme_dict.items()}\n",
    "    return grapheme_dict, inv_grapheme_dict, words, labels, lengths\n",
    "\n",
    "\n",
    "#Backup, Imranul's 1st\n",
    "###################################Decodes list of characters from it's numeric mapping################\n",
    "\n",
    "\n",
    "def decode_prediction(preds, inv_grapheme_dict, raw = False):\n",
    "    grapheme_list = []\n",
    "    pred_list = []\n",
    "    for i in range(len(preds)):\n",
    "        if preds[i] != 0 and preds[i] != 1 and (not (i > 0 and preds[i - 1] == preds[i])):\n",
    "            grapheme_list.append(inv_grapheme_dict.get(preds[i]))\n",
    "            pred_list.append(preds[i])\n",
    "\n",
    "    return pred_list, ''.join(grapheme_list)\n",
    "\n",
    "\n",
    "\n",
    "####################################Extracts graphemes(letters) from dataset and throws away useless ones####\n",
    "###################You can try to understand it but will require Bangla Grammer skills and good logic ######\n",
    "############################################################################################################\n",
    "\n",
    "\n",
    "def extract_graphemes(word):\n",
    "    support_chars = ['্', 'ং', 'ঃ', 'ঁ', 'ি', 'ু', 'ূ', 'ৃ', 'ে', 'ো', 'ৌ' ,'ী', 'া', 'ে', 'ৈ']\n",
    "    ref_chars = [ '্য', '্র', 'র্', 'য', 'র']\n",
    "    unicode_garbage = ['\\x02', '\\x03', '\\x06', '\\x08', '\\x10', '\\x12', '&', '¡',\n",
    "                        '¤', '¥', '¦', '©', '¬', '\\xad', '®', '¯', 'Ä', 'Í', 'ä', 'æ', 'è', 'ø', 'ÿ',\n",
    "                        'œ', 'š', 'Ÿ', 'ƒ', 'β', '॥', '\\u09e4', '\\u200b', '\\u200d', '\\u200f', '\\uf020',\n",
    "                        '\\uf02d', '�', '\\u200b', '\\u200c', '\\u09e5']\n",
    "    \n",
    "    chars = []\n",
    "    i = 0\n",
    "    prev_ref = False\n",
    "\n",
    "    while(i < len(word)):\n",
    "        if word[i] != support_chars[0] and word[i] not in unicode_garbage:\n",
    "            if i+1 < len(word):\n",
    "                if word[i+1] != support_chars[0]:\n",
    "                    if word[i+1] == ref_chars[-1] and i+2 < len(word):\n",
    "                        if word[i+2] == support_chars[0]:\n",
    "                            chars.append(word[i])\n",
    "                            chars.append(ref_chars[2])\n",
    "                            i += 2\n",
    "                            prev_ref = True\n",
    "                        else:\n",
    "                            chars.append(word[i])\n",
    "                            i += 1\n",
    "                    else:\n",
    "                        chars.append(word[i])\n",
    "                        i += 1\n",
    "                elif word[i+1] == support_chars[0] and word[i] not in support_chars[0:]:\n",
    "                    previous = False\n",
    "                    isSupport = True\n",
    "                    idx = i+1\n",
    "                    if idx<len(word):\n",
    "                        while(isSupport):\n",
    "                            if idx<len(word):\n",
    "                                # print(word[i], word[idx], i, idx)\n",
    "                                if (word[idx] == support_chars[0] or word[idx] == ref_chars[4]) and idx+1 < len(word):\n",
    "                                    if word[idx] == support_chars[0] and word[idx-1] == ref_chars[-1]:\n",
    "                                        if not previous:\n",
    "                                            if i != idx:\n",
    "                                                chars.append(word[i:(idx-1)])\n",
    "                                            chars.append(ref_chars[2])\n",
    "                                            idx += 1\n",
    "                                            i = idx\n",
    "                                            continue\n",
    "                                    if word[idx] == ref_chars[-1]:\n",
    "\n",
    "                                        if word[idx+1] != support_chars[0]:\n",
    "                                            chars.append(ref_chars[-1])\n",
    "                                        idx += 1\n",
    "                                        i = idx\n",
    "                                        continue\n",
    "                                    if word[idx+1] == ref_chars[3]:\n",
    "                                        if i != idx:\n",
    "                                            chars.append(word[i:idx])\n",
    "                                        chars.append(ref_chars[0])\n",
    "                                        idx += 2\n",
    "                                        i = idx\n",
    "                                        # print(i)\n",
    "                                        # print(idx)\n",
    "                                        continue\n",
    "                                    if word[idx+1] == ref_chars[4]:\n",
    "                                        # print(chars)\n",
    "                                        if i != idx:\n",
    "                                            chars.append(word[i:idx])\n",
    "                                        chars.append(ref_chars[1])\n",
    "                                        idx += 2\n",
    "                                        i = idx\n",
    "                                        previous = True\n",
    "                                        continue\n",
    "                                    if word[idx+1] == '\\u200c':\n",
    "                                        if i != idx:\n",
    "                                            chars.append(word[i:idx])\n",
    "                                        i = idx+2\n",
    "                                        isSupport = False\n",
    "\n",
    "                                    idx += 2\n",
    "                                else:\n",
    "                                    isSupport= False\n",
    "                            else:\n",
    "                                isSupport = False\n",
    "                    if i != idx:\n",
    "                        chars.append(word[i:idx])\n",
    "                    i = idx\n",
    "                else:\n",
    "                    if word[i] in support_chars[0:]:\n",
    "                        chars.append(word[i])\n",
    "                    i += 2\n",
    "            else:\n",
    "                chars.append(word[i])\n",
    "                i += 1\n",
    "        else:\n",
    "            if word[i]== support_chars[0]:\n",
    "                if prev_ref:\n",
    "                    prev_ref = False\n",
    "                    i += 1\n",
    "                    continue\n",
    "                chars.append(word[i])\n",
    "                i+=1\n",
    "                continue\n",
    "            else:\n",
    "                i+=1\n",
    "                continue\n",
    "        \n",
    "    return chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9y5ibi5xetrq"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "t2pknG1Metrv"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "e5Zzouaketry"
   },
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ou9Ov8d9etry"
   },
   "source": [
    "### Functions that will take your dataset and wrangle it into a numeric version\n",
    "### Your computer can understand\n",
    "### Also provided is a helper function that will pad your encoded version of\n",
    "### dataset. Also get all necessary labels. Required for variable length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pvjzrFV6etr0"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qCU4Ams_etr1"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xRSaTziietr3"
   },
   "outputs": [],
   "source": [
    "#necessary imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ve-qT8JBetr6"
   },
   "outputs": [],
   "source": [
    "import torch.utils.data as data\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LKoYKmseetr9"
   },
   "outputs": [],
   "source": [
    "\n",
    "#########OCR Dataset Class###################################################\n",
    "#########Helper functions to convert your images#############################\n",
    "#########to the desired format###############################################\n",
    "\n",
    "class  OCRDataset(data.Dataset):\n",
    "    def __init__(self, img_dir):\n",
    "\n",
    "        self.img_dir = img_dir\n",
    "        # self.text_dir = text_dir\n",
    "        self.inp_h = 32\n",
    "        self.inp_w = 128\n",
    "        self.mean = np.array(0.588, dtype=np.float32)\n",
    "        self.std = np.array(0.193, dtype=np.float32)\n",
    "        self.images = sorted(os.listdir(img_dir), key=lambda x: int(x.split('_')[0]))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        \n",
    "        \n",
    "        img_name = self.images[idx]\n",
    "        # print(img_name)\n",
    "        img_path = os.path.join(self.img_dir, img_name)\n",
    "        \n",
    "        stream = open(img_path, \"rb\")\n",
    "        bytes = bytearray(stream.read())\n",
    "        numpyarray = np.asarray(bytes, dtype=np.uint8)\n",
    "        img = cv2.imdecode(numpyarray, cv2.IMREAD_UNCHANGED)\n",
    "        #img = cv2.imread(os.path.join(self.img_dir, img_name))                     \n",
    "        # print(img)\n",
    "        \n",
    "        #convert to greyscale\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        img_h, img_w = img.shape\n",
    "        \n",
    "        #Resize to input size for network (32,128,1)\n",
    "        img = cv2.resize(img, (0,0), fx=self.inp_w / img_w, fy=self.inp_h / img_h, interpolation=cv2.INTER_CUBIC)\n",
    "        img = np.reshape(img, (self.inp_h, self.inp_w, 1))\n",
    "\n",
    "        #Normalize by mean and standard deviation\n",
    "        img = img.astype(np.float32)\n",
    "        # img = (img/255. - self.mean) / self.std\n",
    "        \n",
    "        #Reshape to tensor format supported by Pytorch (C, H, W)\n",
    "        img = img.transpose([2, 0, 1])\n",
    "        \n",
    "        return img, img_name, idx\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "c34Oe5GqetsC"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, embed_size, heads):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        self.embed_size = embed_size\n",
    "        self.heads = heads\n",
    "        self.head_dim = embed_size\n",
    "        \n",
    "        assert(self.head_dim * heads == embed_size) , \"embed size must be divisable by heads\"\n",
    "        \n",
    "        self.values = nn.Linear(self.head_dim, self.head_dim, bias = False)\n",
    "        self.keys = nn.Linear(self.head_dim, self.head_dim, bias = False)\n",
    "        self.queries = nn.Linear(self.head_dim, self.head_dim, bias = False)\n",
    "        \n",
    "        self.fc_out = nn.Linear(heads * self.head_dim, embed_size)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xVyYNFU4etsF"
   },
   "outputs": [],
   "source": [
    "############Helper function to pad your label lists######################################\n",
    "############Get all necessary labels from your datasets#################################\n",
    "\n",
    "def get_padded_labels(idxs, grapheme_dict, inv_grapheme_dict, words, labels, lengths):\n",
    "    batch_labels = []\n",
    "    batch_lengths = []\n",
    "    batch_words = []\n",
    "    maxlen = 0\n",
    "    for idx in idxs:\n",
    "        batch_labels.append(labels[idx])\n",
    "        batch_words.append(words[idx])\n",
    "        batch_lengths.append(len(labels[idx]))\n",
    "        maxlen = max(len(labels[idx]), maxlen)\n",
    "    \n",
    "    #changed [1]*(maxlen-len(batch_labels[i])) to [0]*(maxlen-len(batch_labels[i]))\n",
    "    #Alls good\n",
    "    for i in range(len(batch_labels)):\n",
    "        batch_labels[i] = batch_labels[i] + [1]*(maxlen-len(batch_labels[i]))\n",
    "\n",
    "    return batch_words, batch_labels, batch_lengths, inv_grapheme_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "35rF-gCfetsJ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2vvs2e8WetsM"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vtO5PADpetsQ"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YYXOMqgdetsR"
   },
   "source": [
    "# The Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GOgkn7PNetsS"
   },
   "source": [
    " ### As said earlier the model is a collection of layers,\n",
    " \n",
    " ### A Convolutional network without it's fully connected layer\n",
    " ### What it does is extract necessary features from the image\n",
    " ### like which parts are important and such\n",
    " \n",
    " ### After we have the necessary features in rectangular form\n",
    " ### as word images are in rectangular form.\n",
    " ### we send it to a Bi-LSTM (Specialized complicated RNN)\n",
    " ### to read the image left to right and right to left \n",
    " ### and model the sequence\n",
    " \n",
    " ### The sequence predicted normally contains a lot of \n",
    " ### redundancy and repeations. The reason which\n",
    " ### we use a CTC loss to predict a variable length\n",
    " ### prediction with control of repeations.\n",
    " \n",
    " ### Remember, this is a baseline solution though.\n",
    " ### There are many better methodologies than this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "u98Qi9O2etsS"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EDwUiRGEetsW"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6VOak1wGetsY"
   },
   "outputs": [],
   "source": [
    "##Necessary Imports\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lxYcr7d4etsb"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_uSmqkYUetsd"
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_uSmqkYUetsd"
   },
   "outputs": [],
   "source": [
    "\n",
    "#######################Some module that predicts sequences from the compacted feature rich version of ########\n",
    "#######################image################################################################################\n",
    "\n",
    "\n",
    "class BidirectionalLSTM(nn.Module):\n",
    "    # Inputs hidden units Out\n",
    "    def __init__(self, nIn, nHidden, nOut):\n",
    "        super(BidirectionalLSTM, self).__init__()\n",
    "\n",
    "        self.rnn = nn.LSTM(nIn, nHidden, bidirectional=True)\n",
    "        self.embedding = nn.Linear(nHidden * 2, nOut)\n",
    "\n",
    "    def forward(self, input):\n",
    "        recurrent, _ = self.rnn(input)\n",
    "        T, b, h = recurrent.size()\n",
    "        t_rec = recurrent.view(T * b, h)\n",
    "\n",
    "        output = self.embedding(t_rec)  # [T * b, nOut]\n",
    "        output = output.view(T, b, -1)\n",
    "        # print(output.shape)\n",
    "        return output\n",
    "\n",
    "\n",
    "#######################Forward part is the real architecture. ################################\n",
    "#######################The functions before that are used to #################################\n",
    "#######################declare the feature extracting CNN    #################################\n",
    "\n",
    "class CRNN(nn.Module):\n",
    "    def __init__(self, imgH, nc, nclass, nh, n_rnn=2, leakyRelu=False):\n",
    "        super(CRNN, self).__init__()\n",
    "        assert imgH % 16 == 0, 'imgH has to be a multiple of 16'\n",
    "        \n",
    "        \n",
    "        #########################Convolutional Backbone Declaration############################\n",
    "        \n",
    "        \n",
    "        ###kernel value for every layer\n",
    "        ks = [3, 3, 3, 3, 3, 3, 3, 3, 2]\n",
    "        \n",
    "        ###padding value for every layer\n",
    "        ps = [1, 1, 1, 1, 1, 1, 1, 1, 0]\n",
    "        \n",
    "        ###stride value for every layer\n",
    "        ss = [1,1,1, 1, 1, 1, 1, 1, 1]\n",
    "        \n",
    "        ###channel value for every layer\n",
    "        nm = [32, 64, 128, 128,256,256, 512, 512, 512]\n",
    "        # w-k+2p/s\n",
    "\n",
    "        ##Sequential is good way to list layers one after another.\n",
    "        ##To actually understand the syntax of Pytorch. we would\n",
    "        ## suggest learning two things: \n",
    "        \n",
    "        ## Syntax of Python classes and objects\n",
    "        ## https://www.youtube.com/watch?v=wfcWRAxRVBA&list=PLBZBJbE_rGRWeh5mIBhD-hhDwSEDxogDg&index=9\n",
    "        \n",
    "        ## For Pytorch, there's the official documents\n",
    "        ## But this medium article is enough to be honest\n",
    "        ## https://towardsdatascience.com/pytorch-how-and-when-to-use-module-sequential-modulelist-and-moduledict-7a54597b5f17\n",
    "\n",
    "        \n",
    "        \n",
    "        cnn = nn.Sequential()\n",
    "        \n",
    "        \n",
    "        def convRelu(i, batchNormalization=False):\n",
    "            nIn = nc if i == 0 else nm[i - 1]\n",
    "            nOut = nm[i]\n",
    "            #print(nIn, nOut)\n",
    "            cnn.add_module('conv{0}'.format(i), nn.Conv2d(nIn, nOut, ks[i], ss[i], ps[i]))\n",
    "            if batchNormalization:\n",
    "                cnn.add_module('batchnorm{0}'.format(i), nn.BatchNorm2d(nOut))\n",
    "            if leakyRelu:\n",
    "                cnn.add_module('relu{0}'.format(i), nn.LeakyReLU(0.2, inplace=True))\n",
    "            else:\n",
    "                cnn.add_module('relu{0}'.format(i), nn.ReLU(True))\n",
    "\n",
    "        convRelu(0)\n",
    "        \n",
    "        ##Output shape is 512X1X33\n",
    "        ##(Batch X Height X Width)\n",
    "        # bs, 32, 128\n",
    "        # 512, 16, 1\n",
    "        \n",
    "        \n",
    "        cnn.add_module('pooling{0}'.format(0), nn.MaxPool2d(2, 2)) \n",
    "        convRelu(1)\n",
    "        cnn.add_module('pooling{0}'.format(1), nn.MaxPool2d(2, 2)) \n",
    "        convRelu(2)\n",
    "        cnn.add_module('pooling{0}'.format(1), nn.MaxPool2d(2, 2)) \n",
    "        convRelu(3,True)\n",
    "        cnn.add_module('pooling{0}'.format(2), nn.MaxPool2d((2, 2), (2, 1), (0, 1)))\n",
    "        convRelu(4)\n",
    "        cnn.add_module('pooling{0}'.format(1), nn.MaxPool2d(2, 2)) \n",
    "        convRelu(5)\n",
    "        cnn.add_module('pooling{0}'.format(3), nn.MaxPool2d((2, 2), (2, 1), (0, 1))) \n",
    "        convRelu(6, True)  # 512x1x33\n",
    "        convRelu(7)\n",
    "        cnn.add_module('pooling{0}'.format(3), nn.MaxPool2d((2, 2), (2, 1), (0, 1))) \n",
    "        convRelu(8,True)\n",
    "\n",
    "        self.cnn = cnn\n",
    "        print(cnn)\n",
    "        \n",
    "        #########################Convolutional Backbone Declaration############################\n",
    "        \n",
    "        \n",
    "        \n",
    "        #########################Inherit LSTM function from LSTM function######################\n",
    "        \n",
    "        self.rnn = nn.Sequential(\n",
    "            BidirectionalLSTM(512, nh, nh),\n",
    "            BidirectionalLSTM(nh, nh, nclass))\n",
    "        \n",
    "        #########################Inherit LSTM function from LSTM function######################\n",
    "\n",
    "    def forward(self, input):\n",
    "\n",
    "        #input is the input image in (Batch, Channel, Height, Width) form\n",
    "        \n",
    "        \n",
    "        #conv = Feature extracted by Convolutional Network\n",
    "        conv = self.cnn(input)\n",
    "        \n",
    "        \n",
    "        #######convert feature(conv) so LSTM can read it##################\n",
    "        \n",
    "        b, c, h, w = conv.size()\n",
    "        #print(conv.shape)\n",
    "        #make sure height is 1, we will predicting along the sequnce\n",
    "        assert h == 1, \"the height of conv must be 1\"\n",
    "        \n",
    "        \n",
    "        conv = conv.squeeze(2) # b *512 * width\n",
    "        conv = conv.permute(2, 0, 1)  # [w, b, c]\n",
    "        \n",
    "        #############################################################\n",
    "        \n",
    "        \n",
    "        #############Send to LSTM then###############################\n",
    "        #############softmax it to get ##############################\n",
    "        #############probability between 0 and 1#####################\n",
    "        ###Softmax across dimension 2 because it will have###########\n",
    "        ####288 possible labels and they will have values############\n",
    "        ####(probability distribution)(between 0 and 1)##############\n",
    "        output = F.log_softmax(self.rnn(conv), dim=2)\n",
    "\n",
    "        return output\n",
    "\n",
    "    \n",
    "###########weight initialization helps model achieve better###########\n",
    "#############gradients gradually, experiment with HE intialization####\n",
    "#############Xavier init etc, if possible#############################\n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        m.weight.data.normal_(0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        m.weight.data.normal_(1.0, 0.02)\n",
    "        m.bias.data.fill_(0)\n",
    "\n",
    "        \n",
    "#####Send model after intializing weight##############################        \n",
    "def get_crnn():\n",
    "    \n",
    "    #(Initial Image Height, Feature Height, Labels, LSTM hidden Layer)\n",
    "    model = CRNN(32, 1, 288, 256)\n",
    "    model.apply(weights_init)\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mwd-1I9tetsg"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "28BMnMbOetsj"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y3G2uy2letsm"
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PzmOXMWCetsm"
   },
   "source": [
    " ### The moment you have been waiting for,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Msy7xQtSetsm"
   },
   "source": [
    " ### (IMPORTANT NOTE). Remember to change the directory of the dataset to the place the dataset is located. An example is given below in the paths locations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YGDsIbHZetsn"
   },
   "source": [
    "### Run the functions then understand the functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "E8oPTFxketso"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zunetkNTetsq"
   },
   "outputs": [],
   "source": [
    "##Necessary Imports\n",
    "\n",
    "\n",
    "import torch\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import torch.optim as optim\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "import json\n",
    "import pickle\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8v18hllQetsu",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (conv0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (relu0): ReLU(inplace=True)\n",
      "  (pooling0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv1): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (relu1): ReLU(inplace=True)\n",
      "  (pooling1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv2): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (relu2): ReLU(inplace=True)\n",
      "  (conv3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (batchnorm3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu3): ReLU(inplace=True)\n",
      "  (pooling2): MaxPool2d(kernel_size=(2, 2), stride=(2, 1), padding=(0, 1), dilation=1, ceil_mode=False)\n",
      "  (conv4): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (relu4): ReLU(inplace=True)\n",
      "  (conv5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (relu5): ReLU(inplace=True)\n",
      "  (pooling3): MaxPool2d(kernel_size=(2, 2), stride=(2, 1), padding=(0, 1), dilation=1, ceil_mode=False)\n",
      "  (conv6): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (batchnorm6): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu6): ReLU(inplace=True)\n",
      "  (conv7): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (relu7): ReLU(inplace=True)\n",
      "  (conv8): Conv2d(512, 512, kernel_size=(2, 2), stride=(1, 1))\n",
      "  (batchnorm8): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu8): ReLU(inplace=True)\n",
      ")\n",
      "cnn.conv0.weight 288\n",
      "cnn.conv0.bias 32\n",
      "cnn.conv1.weight 18432\n",
      "cnn.conv1.bias 64\n",
      "cnn.conv2.weight 73728\n",
      "cnn.conv2.bias 128\n",
      "cnn.conv3.weight 147456\n",
      "cnn.conv3.bias 128\n",
      "cnn.batchnorm3.weight 128\n",
      "cnn.batchnorm3.bias 128\n",
      "cnn.conv4.weight 294912\n",
      "cnn.conv4.bias 256\n",
      "cnn.conv5.weight 589824\n",
      "cnn.conv5.bias 256\n",
      "cnn.conv6.weight 1179648\n",
      "cnn.conv6.bias 512\n",
      "cnn.batchnorm6.weight 512\n",
      "cnn.batchnorm6.bias 512\n",
      "cnn.conv7.weight 2359296\n",
      "cnn.conv7.bias 512\n",
      "cnn.conv8.weight 1048576\n",
      "cnn.conv8.bias 512\n",
      "cnn.batchnorm8.weight 512\n",
      "cnn.batchnorm8.bias 512\n",
      "rnn.0.rnn.weight_ih_l0 524288\n",
      "rnn.0.rnn.weight_hh_l0 262144\n",
      "rnn.0.rnn.bias_ih_l0 1024\n",
      "rnn.0.rnn.bias_hh_l0 1024\n",
      "rnn.0.rnn.weight_ih_l0_reverse 524288\n",
      "rnn.0.rnn.weight_hh_l0_reverse 262144\n",
      "rnn.0.rnn.bias_ih_l0_reverse 1024\n",
      "rnn.0.rnn.bias_hh_l0_reverse 1024\n",
      "rnn.0.embedding.weight 131072\n",
      "rnn.0.embedding.bias 256\n",
      "rnn.1.rnn.weight_ih_l0 262144\n",
      "rnn.1.rnn.weight_hh_l0 262144\n",
      "rnn.1.rnn.bias_ih_l0 1024\n",
      "rnn.1.rnn.bias_hh_l0 1024\n",
      "rnn.1.rnn.weight_ih_l0_reverse 262144\n",
      "rnn.1.rnn.weight_hh_l0_reverse 262144\n",
      "rnn.1.rnn.bias_ih_l0_reverse 1024\n",
      "rnn.1.rnn.bias_hh_l0_reverse 1024\n",
      "rnn.1.embedding.weight 147456\n",
      "rnn.1.embedding.bias 288\n",
      "8625568\n"
     ]
    }
   ],
   "source": [
    "#Retrieve Model and print model\n",
    "\n",
    "model = get_crnn()\n",
    "model = model.cuda()\n",
    "#print(model)\n",
    "\n",
    "#########Counting Model Parameters#######################\n",
    "\n",
    "#counting parameters\n",
    "def count_parameters(model):\n",
    "    for name, param in model.named_parameters():\n",
    "        if param.requires_grad:\n",
    "            print(name, param.numel())\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "\n",
    "print(count_parameters(model))\n",
    "##########################################################\n",
    "\n",
    "\n",
    "###############Continuation of training###################\n",
    "\n",
    "#Path to model, if you encounter loadshedding.\n",
    "#And want to continue training\n",
    "#model.load_state_dict(torch.load('path/to/model.pth'))\n",
    "\n",
    "##########################################################\n",
    "\n",
    "\n",
    "#########################Loss function decalaration############################\n",
    "\n",
    "\n",
    "#Zero inifinity problem, gradient collapses to zero, when 287 labels\n",
    "criterion = torch.nn.CTCLoss(blank =0, reduction='mean', zero_infinity = True)\n",
    "criterion = criterion.cuda()\n",
    "#[1,1,1,1,1,10,10,4,4]\n",
    "#[]\n",
    "#[1,10,4]\n",
    "\n",
    "###############################################################################\n",
    "\n",
    "\n",
    "\n",
    "#################Extract and Preprocess dataset################################\n",
    "\n",
    "##Example::\n",
    "##ocr_dataset = OCRDataset('/home/imr555/Desktop/Apurba_Job/Day_14/wlbocrv3/out_50000')\n",
    "ocr_dataset = OCRDataset('./data/out_50000/')\n",
    "\n",
    "\n",
    "##Example::\n",
    "##grapheme_dict_i, inv_grapheme_dict_i, words_i, labels_i, lengths_i = preprocess_data('/home/imr555/Desktop/Apurba_Job/Day_14/wlbocrv3/out_50000')\n",
    "grapheme_dict_i, inv_grapheme_dict_i, words_i, labels_i, lengths_i = preprocess_data('./data/out_50000/')\n",
    "\n",
    "# print(grapheme_dict_i)\n",
    "#print(len(grapheme_dict_i))\n",
    "#sanity check\n",
    "#print(words_i)\n",
    "#print(labels_i)\n",
    "\n",
    "#################################################################################\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#######################Setting validation and train split########################\n",
    "\n",
    "\n",
    "validation_split = .1\n",
    "shuffle_dataset = True\n",
    "random_seed= 42\n",
    "\n",
    "#seeding for reproducability\n",
    "torch.manual_seed(random_seed)\n",
    "\n",
    "# Creating data indices for training and validation splits\n",
    "dataset_size = len(ocr_dataset)\n",
    "indices = list(range(dataset_size))\n",
    "split = int(np.floor(validation_split * dataset_size))\n",
    "if shuffle_dataset :\n",
    "    np.random.seed(random_seed)\n",
    "    np.random.shuffle(indices)\n",
    "train_indices, val_indices = indices[split:], indices[:split]\n",
    "\n",
    "train_sampler = SubsetRandomSampler(train_indices)\n",
    "valid_sampler = SubsetRandomSampler(val_indices)\n",
    "\n",
    "#Batch Size variable (Decrease it if you hit memory error, or increase it for faster train)\n",
    "train_batch_s = 28\n",
    "valid_batch_s = 28\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(ocr_dataset, batch_size= train_batch_s, \n",
    "                                           sampler=train_sampler)\n",
    "validation_loader = torch.utils.data.DataLoader(ocr_dataset, batch_size= valid_batch_s,\n",
    "                                                sampler=valid_sampler)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qg7rL4Dmetsx"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oFOAYS0Retsz"
   },
   "outputs": [],
   "source": [
    "#####Train and validate already########\n",
    "\n",
    "#####There's necessary stuff###########\n",
    "#####That will get printed#############\n",
    "####every epoch. Feel free to add more#\n",
    "#######################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zg3NF-R8ets1"
   },
   "outputs": [],
   "source": [
    "def train(metrics1, metrics2):\n",
    "    \n",
    "    ###Set epoch number here\n",
    "    for epoch in range(15):\n",
    "        print(\"***Epoch: {}***\".format(epoch))\n",
    "        batch_loss = 0\n",
    "        #change this for epoch\n",
    "        if epoch>7:\n",
    "            optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=0.0003)\n",
    "        else:\n",
    "            optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=0.001)\n",
    "        for i, (inp, img_names, idx) in enumerate(tqdm(train_loader)):\n",
    "            inp = inp.cuda()\n",
    "            batch_size = inp.size(0)\n",
    "            idxs = idx.detach().numpy()\n",
    "            img_names = list(img_names)\n",
    "            words, labels, labels_size, inv_grapheme_dict = get_padded_labels(idxs, grapheme_dict_i, inv_grapheme_dict_i, words_i, labels_i, lengths_i)\n",
    "            #print(inp.size(0))\n",
    "            preds = model(inp)\n",
    "            labels = torch.tensor(labels, dtype=torch.long)\n",
    "            labels = labels.cuda()\n",
    "            labels_size = torch.tensor(labels_size, dtype=torch.long)\n",
    "            labels_size = labels_size.cuda()\n",
    "            preds_size = torch.tensor([preds.size(0)] * batch_size, dtype=torch.long)\n",
    "            preds_size = preds_size.cuda()\n",
    "            loss = criterion(preds, labels, preds_size, labels_size)\n",
    "            #print(loss.item())\n",
    "            batch_loss += loss.item()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        train_loss = batch_loss/train_batch_s\n",
    "        print(\"Epoch Training loss: \", train_loss) #batch_size denominator 32\n",
    "        \n",
    "\n",
    "        print(\"\\n\")\n",
    "        validate(epoch, metrics1, metrics2, train_loss)\n",
    "        #if epoch%1 == 0:\n",
    "        torch.save(model.state_dict(), 'epoch{}.pth'.format(epoch))\n",
    "\n",
    "def validate(epoch, metrics1, metrics2, train_loss):\n",
    "    with torch.no_grad():\n",
    "        y_true = []\n",
    "        y_pred = []\n",
    "        pred_ = []\n",
    "        label_ = []\n",
    "\n",
    "        total_wer = 0\n",
    "\n",
    "        print(\"***Epoch: {}***\".format(epoch))\n",
    "        batch_loss = 0\n",
    "        for i, (inp, img_names, idx) in enumerate(tqdm(validation_loader)):\n",
    "            inp = inp.cuda()\n",
    "            batch_size = inp.size(0)\n",
    "            idxs = idx.detach().numpy()\n",
    "            img_names = list(img_names)\n",
    "            words, labels, labels_size, inv_grapheme_dict = get_padded_labels(idxs, grapheme_dict_i, inv_grapheme_dict_i, words_i, labels_i, lengths_i)\n",
    "            preds = model(inp)\n",
    "            labels = torch.tensor(labels, dtype=torch.long)\n",
    "            labels = labels.cuda()\n",
    "            labels_size = torch.tensor(labels_size, dtype=torch.long)\n",
    "            labels_size = labels_size.cuda()\n",
    "            preds_size = torch.tensor([preds.size(0)] * batch_size, dtype=torch.long) \n",
    "            preds_size = preds_size.cuda()\n",
    "\n",
    "            #validation loss\n",
    "            loss = criterion(preds, labels, preds_size, labels_size)\n",
    "            #print(loss)\n",
    "            batch_loss += loss.item()\n",
    "            #print(loss.item())\n",
    "\n",
    "            _, preds = preds.max(2)\n",
    "            preds = preds.transpose(1, 0).contiguous().detach().cpu().numpy()\n",
    "            labels = labels.detach().cpu().numpy()\n",
    "            \n",
    "            for i in range(len(preds)):\n",
    "                decoded, _ = decode_prediction(preds[i], inv_grapheme_dict)\n",
    "                for x,y in zip(decoded, labels[i]):\n",
    "                    y_pred.append(x)\n",
    "                    y_true.append(y)\n",
    "                _, decoded_pred_ = decode_prediction(preds[i], inv_grapheme_dict)\n",
    "                #print(inv_grapheme_dict)\n",
    "                _, decoded_label_ = decode_prediction(labels[i], inv_grapheme_dict)\n",
    "                #print(decoded_label_)\n",
    "                \n",
    "                pred_.append(decoded_pred_)\n",
    "                label_.append(decoded_label_)\n",
    "\n",
    "        valid_loss = batch_loss/valid_batch_s\n",
    "        print(\"Epoch Validation loss: \", valid_loss) #batch_size denominator 32\n",
    "        print(\"\\n\")\n",
    "        #print(pred_)\n",
    "        #print(label_)\n",
    "        total_wer = compute_wer(pred_, label_)\n",
    "        print(\"Total_Word_Error_Rate: %.4f\" % total_wer)\n",
    "\n",
    "        #change in number of labels\n",
    "        # [5,6,3,2]\n",
    "        # [2,6,3,1]\n",
    "        report = classification_report(y_true, y_pred, labels = np.arange(1,288), zero_division=0)\n",
    "        f1_micro = f1_score(y_true, y_pred, average = 'micro', zero_division=0)\n",
    "        f1_macro = f1_score(y_true, y_pred, average = 'macro', zero_division=0)\n",
    "        accuracy = accuracy_score(y_true, y_pred)\n",
    "        \n",
    "\n",
    "        #Absolute word matching\n",
    "        abs_correct = absolute_word_match(pred_, label_)\n",
    "\n",
    "\n",
    "\n",
    "        with open('Results__Report_epoch{}.txt'.format(epoch), 'w') as fout2:\n",
    "            fout2.write(report)\n",
    "\n",
    "\n",
    "        with open('results.txt', 'w', encoding='utf8') as fout:\n",
    "            for x,y in zip(pred_, label_):\n",
    "                #x.encode('utf8')\n",
    "                #y.encode('utf8')\n",
    "                fout.write(\"True: {}\".format(y))\n",
    "                fout.write(\"\\n\")\n",
    "                fout.write(\"Pred: {}\".format(x))\n",
    "                fout.write(\"\\n\\n\")\n",
    "        print(\"Accuracy: %.4f\" % accuracy)\n",
    "        print(\"F1 Micro Score: %.4f\" % f1_micro)\n",
    "        print(\"F1 Macro Score: %.4f\" % f1_macro)\n",
    "        print(\"\\n\")\n",
    "\n",
    "        ################################################JSON Dumps\n",
    "        metrics1['epoch'].append(epoch)\n",
    "        metrics1['accuracy'].append(accuracy)\n",
    "        metrics1['train_loss'].append(train_loss)\n",
    "        metrics1['valid_loss'].append(valid_loss)\n",
    "        metrics1['total_wer'].append(total_wer)\n",
    "        metrics1['f1_micro'].append(f1_micro)\n",
    "        metrics1['f1_macro'].append(f1_macro)\n",
    "        metrics1['absolute_word_correct'].append(abs_correct)\n",
    "\n",
    "        json.dump( metrics1, open( \"metrics(general).json\", 'w' ))\n",
    "\n",
    "        metrics2['epoch'].append(epoch)\n",
    "        metrics2['report'].append(report)\n",
    "\n",
    "        json.dump( metrics2, open( \"metrics(report).json\", 'w' ))\n",
    "\n",
    "\n",
    "        print(\"End of Epoch {}\".format(epoch))\n",
    "        print(\"\\n\\n\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-Q7UYJNIets3"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Lg-uVSi6ets6"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "E1ovTQDzets7"
   },
   "outputs": [],
   "source": [
    "###UP UP AND AWAAAAYYYYYY#######"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ohhfLFwAets9",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                        | 0/25714 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***Epoch: 0***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 25714/25714 [48:34<00:00,  8.82it/s]\n",
      "  0%|                                                                                 | 1/2858 [00:00<04:47,  9.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch Training loss:  656.7747793409175\n",
      "\n",
      "\n",
      "***Epoch: 0***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 2858/2858 [03:35<00:00, 13.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch Validation loss:  13.444339481182396\n",
      "\n",
      "\n",
      "Total_Word_Error_Rate: 0.4232\n",
      "Absolute word match count is 62634\n",
      "Accuracy: 0.9108\n",
      "F1 Micro Score: 0.9108\n",
      "F1 Macro Score: 0.6822\n",
      "\n",
      "\n",
      "End of Epoch 0\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                | 1/25714 [00:00<46:35,  9.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***Epoch: 1***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 25714/25714 [47:06<00:00,  9.10it/s]\n",
      "  0%|                                                                                 | 2/2858 [00:00<03:47, 12.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch Training loss:  113.19398562570235\n",
      "\n",
      "\n",
      "***Epoch: 1***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 2858/2858 [02:46<00:00, 17.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch Validation loss:  10.181263186336894\n",
      "\n",
      "\n",
      "Total_Word_Error_Rate: 0.3250\n",
      "Absolute word match count is 66392\n",
      "Accuracy: 0.9270\n",
      "F1 Micro Score: 0.9270\n",
      "F1 Macro Score: 0.8010\n",
      "\n",
      "\n",
      "End of Epoch 1\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                | 2/25714 [00:00<38:15, 11.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***Epoch: 2***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 25714/25714 [46:56<00:00,  9.13it/s]\n",
      "  0%|                                                                                 | 2/2858 [00:00<03:52, 12.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch Training loss:  89.72042522938656\n",
      "\n",
      "\n",
      "***Epoch: 2***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 2858/2858 [02:34<00:00, 18.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch Validation loss:  8.85731574661934\n",
      "\n",
      "\n",
      "Total_Word_Error_Rate: 0.3017\n",
      "Absolute word match count is 67153\n",
      "Accuracy: 0.9333\n",
      "F1 Micro Score: 0.9333\n",
      "F1 Macro Score: 0.8097\n",
      "\n",
      "\n",
      "End of Epoch 2\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                        | 0/25714 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***Epoch: 3***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 25714/25714 [46:57<00:00,  9.13it/s]\n",
      "  0%|                                                                                 | 2/2858 [00:00<03:30, 13.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch Training loss:  79.70741683183171\n",
      "\n",
      "\n",
      "***Epoch: 3***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 2858/2858 [02:26<00:00, 19.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch Validation loss:  7.946532737230882\n",
      "\n",
      "\n",
      "Total_Word_Error_Rate: 0.2752\n",
      "Absolute word match count is 67680\n",
      "Accuracy: 0.9362\n",
      "F1 Micro Score: 0.9362\n",
      "F1 Macro Score: 0.8464\n",
      "\n",
      "\n",
      "End of Epoch 3\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                        | 0/25714 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***Epoch: 4***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 25714/25714 [47:07<00:00,  9.10it/s]\n",
      "  0%|                                                                                 | 2/2858 [00:00<03:50, 12.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch Training loss:  74.88029560084189\n",
      "\n",
      "\n",
      "***Epoch: 4***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 2858/2858 [02:30<00:00, 19.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch Validation loss:  7.491009533139212\n",
      "\n",
      "\n",
      "Total_Word_Error_Rate: 0.2633\n",
      "Absolute word match count is 68290\n",
      "Accuracy: 0.9389\n",
      "F1 Micro Score: 0.9389\n",
      "F1 Macro Score: 0.8585\n",
      "\n",
      "\n",
      "End of Epoch 4\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                        | 0/25714 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***Epoch: 5***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 25714/25714 [47:30<00:00,  9.02it/s]\n",
      "  0%|                                                                                 | 2/2858 [00:00<03:46, 12.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch Training loss:  68.86557530941043\n",
      "\n",
      "\n",
      "***Epoch: 5***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 2858/2858 [02:33<00:00, 18.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch Validation loss:  7.9083258849568665\n",
      "\n",
      "\n",
      "Total_Word_Error_Rate: 0.2707\n",
      "Absolute word match count is 68332\n",
      "Accuracy: 0.9393\n",
      "F1 Micro Score: 0.9393\n",
      "F1 Macro Score: 0.8590\n",
      "\n",
      "\n",
      "End of Epoch 5\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                        | 0/25714 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***Epoch: 6***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 25714/25714 [47:07<00:00,  9.09it/s]\n",
      "  0%|                                                                                 | 2/2858 [00:00<03:40, 12.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch Training loss:  66.97194232669426\n",
      "\n",
      "\n",
      "***Epoch: 6***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 2858/2858 [02:34<00:00, 18.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch Validation loss:  10.06295109952667\n",
      "\n",
      "\n",
      "Total_Word_Error_Rate: 0.3240\n",
      "Absolute word match count is 65599\n",
      "Accuracy: 0.9267\n",
      "F1 Micro Score: 0.9267\n",
      "F1 Macro Score: 0.8048\n",
      "\n",
      "\n",
      "End of Epoch 6\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                        | 0/25714 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***Epoch: 7***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 25714/25714 [47:14<00:00,  9.07it/s]\n",
      "  0%|                                                                                 | 2/2858 [00:00<03:26, 13.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch Training loss:  64.36606352091933\n",
      "\n",
      "\n",
      "***Epoch: 7***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 2858/2858 [02:21<00:00, 20.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch Validation loss:  7.186795037826024\n",
      "\n",
      "\n",
      "Total_Word_Error_Rate: 0.2540\n",
      "Absolute word match count is 68384\n",
      "Accuracy: 0.9406\n",
      "F1 Micro Score: 0.9406\n",
      "F1 Macro Score: 0.8836\n",
      "\n",
      "\n",
      "End of Epoch 7\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                | 2/25714 [00:00<35:15, 12.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***Epoch: 8***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 25714/25714 [46:41<00:00,  9.18it/s]\n",
      "  0%|                                                                                 | 1/2858 [00:00<04:47,  9.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch Training loss:  48.11187203976442\n",
      "\n",
      "\n",
      "***Epoch: 8***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 2858/2858 [02:30<00:00, 19.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch Validation loss:  5.558506248861834\n",
      "\n",
      "\n",
      "Total_Word_Error_Rate: 0.2086\n",
      "Absolute word match count is 70439\n",
      "Accuracy: 0.9503\n",
      "F1 Micro Score: 0.9503\n",
      "F1 Macro Score: 0.9261\n",
      "\n",
      "\n",
      "End of Epoch 8\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                        | 0/25714 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***Epoch: 9***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 25714/25714 [46:36<00:00,  9.20it/s]\n",
      "  0%|                                                                                 | 2/2858 [00:00<03:29, 13.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch Training loss:  44.08002104684004\n",
      "\n",
      "\n",
      "***Epoch: 9***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 2858/2858 [02:29<00:00, 19.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch Validation loss:  5.212175755063072\n",
      "\n",
      "\n",
      "Total_Word_Error_Rate: 0.1984\n",
      "Absolute word match count is 70822\n",
      "Accuracy: 0.9523\n",
      "F1 Micro Score: 0.9523\n",
      "F1 Macro Score: 0.9352\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                        | 0/25714 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of Epoch 9\n",
      "\n",
      "\n",
      "\n",
      "***Epoch: 10***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 25714/25714 [46:40<00:00,  9.18it/s]\n",
      "  0%|                                                                                 | 1/2858 [00:00<04:47,  9.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch Training loss:  41.18458306474453\n",
      "\n",
      "\n",
      "***Epoch: 10***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 2858/2858 [02:26<00:00, 19.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch Validation loss:  4.909692919551162\n",
      "\n",
      "\n",
      "Total_Word_Error_Rate: 0.1915\n",
      "Absolute word match count is 71173\n",
      "Accuracy: 0.9538\n",
      "F1 Micro Score: 0.9538\n",
      "F1 Macro Score: 0.9462\n",
      "\n",
      "\n",
      "End of Epoch 10\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                        | 0/25714 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***Epoch: 11***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 25714/25714 [46:33<00:00,  9.20it/s]\n",
      "  0%|                                                                                 | 2/2858 [00:00<03:27, 13.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch Training loss:  38.814568085250876\n",
      "\n",
      "\n",
      "***Epoch: 11***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 2858/2858 [02:03<00:00, 23.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch Validation loss:  4.721426714856144\n",
      "\n",
      "\n",
      "Total_Word_Error_Rate: 0.1830\n",
      "Absolute word match count is 71266\n",
      "Accuracy: 0.9554\n",
      "F1 Micro Score: 0.9554\n",
      "F1 Macro Score: 0.9464\n",
      "\n",
      "\n",
      "End of Epoch 11\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                        | 0/25714 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***Epoch: 12***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 25714/25714 [46:05<00:00,  9.30it/s]\n",
      "  0%|                                                                                 | 2/2858 [00:00<03:20, 14.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch Training loss:  37.16512888406682\n",
      "\n",
      "\n",
      "***Epoch: 12***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 2858/2858 [02:06<00:00, 22.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch Validation loss:  4.504747735526637\n",
      "\n",
      "\n",
      "Total_Word_Error_Rate: 0.1755\n",
      "Absolute word match count is 71556\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                        | 0/25714 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9574\n",
      "F1 Micro Score: 0.9574\n",
      "F1 Macro Score: 0.9518\n",
      "\n",
      "\n",
      "End of Epoch 12\n",
      "\n",
      "\n",
      "\n",
      "***Epoch: 13***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 25714/25714 [45:56<00:00,  9.33it/s]\n",
      "  0%|                                                                                 | 2/2858 [00:00<03:17, 14.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch Training loss:  35.07620945084688\n",
      "\n",
      "\n",
      "***Epoch: 13***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 2858/2858 [01:58<00:00, 24.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch Validation loss:  4.411415301351814\n",
      "\n",
      "\n",
      "Total_Word_Error_Rate: 0.1704\n",
      "Absolute word match count is 71780\n",
      "Accuracy: 0.9585\n",
      "F1 Micro Score: 0.9585\n",
      "F1 Macro Score: 0.9430\n",
      "\n",
      "\n",
      "End of Epoch 13\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                        | 0/25714 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***Epoch: 14***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 25714/25714 [45:10<00:00,  9.49it/s]\n",
      "  0%|                                                                                 | 2/2858 [00:00<03:37, 13.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch Training loss:  32.93913604941502\n",
      "\n",
      "\n",
      "***Epoch: 14***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 2858/2858 [02:34<00:00, 18.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch Validation loss:  4.126731755864706\n",
      "\n",
      "\n",
      "Total_Word_Error_Rate: 0.1595\n",
      "Absolute word match count is 72235\n",
      "Accuracy: 0.9612\n",
      "F1 Micro Score: 0.9612\n",
      "F1 Macro Score: 0.9480\n",
      "\n",
      "\n",
      "End of Epoch 14\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "metrics1 = {\n",
    "'epoch': [],\n",
    "'accuracy': [],\n",
    "'train_loss': [],\n",
    "'valid_loss': [],\n",
    "'total_wer': [],\n",
    "'f1_micro': [],\n",
    "'f1_macro': [],\n",
    "'absolute_word_correct': [],\n",
    "}\n",
    "\n",
    "metrics2 = {\n",
    "'epoch': [],\n",
    "'report': [],\n",
    "}\n",
    "\n",
    "train(metrics1, metrics2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "R4FDMzfIets_"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7RV8lbhFettB"
   },
   "source": [
    "###########################################################################################################\n",
    "########Things you can try out################################################################################\n",
    "##########1) Try attention modules in place of sequential models like RNN and LSTM#####################################\n",
    "########2) Find augmentation schemes that help the generalization capability of the model#################################\n",
    "########3) Try out different convolutional backbone architectures (HRnet, ResNext, OSA etc) in the model####################\n",
    "########4) Try different stuff in the LSTM module to understand it's weaknesses and how to overcome them###################\n",
    "####### AND if you are up for the challenge A better loss function than CTC#############################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oSK95-CtettC"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tnj_jVRLettF"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IQ6UOsnhettH"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0q9Ls-auettJ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4_hQVh7nettM"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lDP1fOmEettO"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9JJTsBgaettR"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "43O3U_gUettT"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Y67-jOpqettW"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gLWbWIj8ettY"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7VrmRji8ettj"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Td0eqAwmettn"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MUtWNYR1etto"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SYuB_Bxdettq"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MRWV8TaGettr"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wF5T756retts"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JpdKvDgiettu"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qzHyFr90ettw"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bkZBMtvVettx"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "a0WoTHYUettz"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gpNEqDUNett1"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "k8Glf0jQett2"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NfSP5TkQett3"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "znYbccXMett5"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yaQpXIZtett6"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1b6cYzDUett8"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "H_6zPGq0etuA"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eF1GcFOCetuC"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gF7HeW7GetuD"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "M6rd0JGGetuG"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RtGzxzBqetuH"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "PzmOXMWCetsm",
    "Msy7xQtSetsm"
   ],
   "name": "OCR_baseline_CRNN(NSU_APURBA_Lab).ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
